{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_emails(folder):\n",
    "\n",
    "    # Read all email filenames into a list\n",
    "    spam_filenames = [email_text for email_text in sorted(os.listdir(folder))]\n",
    "\n",
    "    \"\"\"\n",
    "    Loop through all files in spam_filenames\n",
    "    Open the file via a file handle. For info: https://www.programiz.com/python-programming/file-operation\n",
    "    Read the contents into a list where each line in the file is an element of the list\n",
    "    Append the email content into the all_contents list\n",
    "    \"\"\"\n",
    "\n",
    "    all_contents = []\n",
    "\n",
    "    for filename in spam_filenames:\n",
    "        with open(os.path.join(folder,filename), encoding=\"Latin-1\") as f:\n",
    "            contents = f.readlines()\n",
    "            all_contents.append(contents)\n",
    "\n",
    "    \"\"\"\n",
    "    Note that all_contents is a list of lists. Here we flatten it into a list of strings\n",
    "    \"\"\"\n",
    "    all_contents_flat = [' '.join(email) for email in all_contents]\n",
    "\n",
    "    # Load all_contents_flat into a pandas dataframe\n",
    "    df = pd.DataFrame({'email_text': all_contents_flat})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_predictions(y_test, y_pred):\n",
    "    print('Accuracy score: ', accuracy_score(y_test, y_pred))\n",
    "    print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    print('\\n Classification Report:\\n',classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following works if your notebook is in a folder that contains a subfolder /enron1/ham/\n",
    "# with in each a series of email data files like 0006.2003-12-18.GP.spam.txt etc\n",
    "SPAM_DIR = './enron1/spam/'\n",
    "HAM_DIR = './enron1/ham/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_df = load_emails(HAM_DIR)\n",
    "ham_df['class'] = 'ham'\n",
    "spam_df = load_emails(SPAM_DIR)\n",
    "spam_df['class'] = 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([ham_df, spam_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3672\n",
       "spam    1500\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: christmas tree farm pictures\\n</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: vastar resources , inc .\\n gary , pro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: calpine daily gas nomination\\n - calp...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: re : issue\\n fyi - see note below - a...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: meter 7268 nov allocation\\n fyi .\\n -...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          email_text class\n",
       "0            Subject: christmas tree farm pictures\\n   ham\n",
       "1  Subject: vastar resources , inc .\\n gary , pro...   ham\n",
       "2  Subject: calpine daily gas nomination\\n - calp...   ham\n",
       "3  Subject: re : issue\\n fyi - see note below - a...   ham\n",
       "4  Subject: meter 7268 nov allocation\\n fyi .\\n -...   ham"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/paolo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/paolo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/paolo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_email_text(email_array):\n",
    "    \n",
    "    # Tokenize as words\n",
    "    processed_array = [word_tokenize(email) for email in email_array]\n",
    "  \n",
    "    # [['a', word'], ['another']]\n",
    "    \n",
    "    # Keep only alphabetic tokens\n",
    "    # Remove stopwords\n",
    "    processed_array = [[word for word in email if (word.isalpha()) & (word not in stopwords.words('english'))] \\\n",
    "                       for email in processed_array]\n",
    "    # [['word'], ['another']]\n",
    "    \n",
    "    # Lemmatize\n",
    "    # For more info: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_array = [[lemmatizer.lemmatize(word) for word in email] for email in processed_array]\n",
    "    \n",
    "    # Flatten back into a string\n",
    "    processed_array = [' '.join(email) for email in processed_array]\n",
    "    \n",
    "    return processed_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [\"A fox and foxes are all just a fox\", \"Another email\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A fox fox fox', 'Another email']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_email_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['clean_email'] = clean_email_text(df['email_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_text</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Subject: guaranteed satisfaction ! great price...</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject guaranteed satisfaction great price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Subject: cdnow shipment confirmation\\n dear da...</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject cdnow shipment confirmation dear daren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>Subject: hpl nom for july 14 , 2000\\n ( see at...</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject hpl nom july see attached file hplo xl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Subject: security warning\\n</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject security warning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>Subject: may golf specials ! !\\n welcome &amp; enj...</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject may golf special welcome enjoy kingwoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             email_text class  \\\n",
       "208   Subject: guaranteed satisfaction ! great price...  spam   \n",
       "339   Subject: cdnow shipment confirmation\\n dear da...   ham   \n",
       "1184  Subject: hpl nom for july 14 , 2000\\n ( see at...   ham   \n",
       "79                          Subject: security warning\\n  spam   \n",
       "708   Subject: may golf specials ! !\\n welcome & enj...   ham   \n",
       "\n",
       "                                            clean_email  \n",
       "208         Subject guaranteed satisfaction great price  \n",
       "339   Subject cdnow shipment confirmation dear daren...  \n",
       "1184  Subject hpl nom july see attached file hplo xl...  \n",
       "79                             Subject security warning  \n",
       "708   Subject may golf special welcome enjoy kingwoo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = (df['class']=='spam').values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_email'], y, test_size=0.33, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model - the blind way\n",
    "\n",
    "#### Feature engineering (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a default naive bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(count_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "y_pred = nb.predict(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9444444444444444\n",
      "\n",
      "Confusion Matrix:\n",
      " [[218   5]\n",
      " [ 14 105]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.98      0.96       223\n",
      "        True       0.95      0.88      0.92       119\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       342\n",
      "   macro avg       0.95      0.93      0.94       342\n",
      "weighted avg       0.94      0.94      0.94       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham words:\n",
      " ['pat' 'pm' 'gary' 'allocation' 'july' 'hou' 'nom' 'volume' 'nomination'\n",
      " 'sitara' 'farmer' 'forwarded' 'ect' 'pec' 'mmbtu' 'hpl' 'daren' 'cc'\n",
      " 'meter' 'enron']\n",
      "Spam words:\n",
      " ['computron' 'viagra' 'cialis' 'de' 'prescription' 'gb' 'xp' 'pain' 'pro'\n",
      " 'soft' 'drug' 'adobe' 'php' 'pill' 'medication' 'ur' 'hot' 'spam'\n",
      " 'removed' 'ali']\n"
     ]
    }
   ],
   "source": [
    "# Rank spamness of words\n",
    "words = np.array(count_vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(count_train.shape[1])\n",
    "probs = nb.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "spam_words = words[ind[:20]]\n",
    "ham_words = words[ind[-20:]]\n",
    "\n",
    "print(\"Ham words:\\n\",ham_words)\n",
    "print(\"Spam words:\\n\",spam_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model - hyperparamter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_candidates = [{'alpha': np.arange(0.01,1.5,.1)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifier object with the classifier and parameter candidates\n",
    "clf = GridSearchCV(estimator=MultinomialNB(), param_grid=parameter_candidates, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'alpha': array([0.01, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91, 1.01,\n",
       "       1.11, 1.21, 1.31, 1.41])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier \n",
    "clf.fit(count_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score  0.9739884393063584 , Best params: {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# View the accuracy score\n",
    "print('Best score ', clf.best_score_, ', Best params:',  clf.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4VPeV4P3v0Y6E2FRiERJUYbAxAZvdICnBxq/Txp1gB2MHYscGkc473e1OT2fc0/GkJ5nH05l0Opnpebtfd7qdRhgSL3HwEpLgbWxsRwJswCy2WRyMFoRYhEBsQmg780fdwmVZoJKqbt0q6XyeR49v3br31ilZ1Kl77+/8jqgqxhhjTF+leB2AMcaY5GaJxBhjTFQskRhjjImKJRJjjDFRsURijDEmKpZIjDHGRMUSiTHGmKhYIjHGGBMVSyTGGGOikuZ1APHg8/nU7/d7HYYxxiSVHTt2nFTV/J62GxCJxO/3s337dq/DMMaYpCIiNZFsZ5e2jDHGRMUSiTHGmKhYIjHGGBMVSyTGGGOi4moiEZHbReSAiBwUke908/x4EXldRPaIyJsiUuisv0VEdoX9tIjIXV32/ScROe9m/MYYY3rmWiIRkVTgMWARMAVYLiJTumz2E2Cdqt4APAr8EEBVN6nqdFWdDiwEmoFXw449GxjuVuzGGGMi5+YZyVzgoKoeUtVW4Bngzi7bTAHecJY3dfM8wFLgJVVthssJ6sfAf3YlamOMMb3iZiIZCxwOe1znrAu3G1jiLH8FyBWRvC7bLAOeDnv8ELBBVY/GMNZuvbjzCE++E9EwamOMGbC8vtn+MLBARHYCC4AjQEfoSREZA0wDXnEeFwD3AP/c04FF5Jsisl1Etjc0NPQpuJc+OMqayuo+7WuMMQOFm4nkCFAU9rjQWXeZqtar6hJVnQF811nXFLbJvcALqtrmPJ4BTAQOikg1kC0iB7t7cVV9XFVnq+rs/PweK/y75fflUNvYTEen9ml/Y4wZCNxMJNuASSISEJEMgpeoNoRvICI+EQnF8AhQ3uUYywm7rKWqv1PV0arqV1U/0KyqE916AxN8ObR2dFLfdNGtlzDGmKTnWiJR1XaC9zNeAfYBz6rqhyLyqIgsdja7GTggIh8Bo4AfhPYXET/BM5q33IqxJ/68HAAOnbzgVQjGGJPwXJ20UVU3Ahu7rPte2PJ6YP0V9q3mszfnu24zOPooryyQH0wk1ScvsODavl0eM8aY/s7rm+0JLX9wJjkZqVTZGYkxxlyRJZKrEBEC+TmWSIwx5ioskfQg4BtsicQYY67CEkkPAnnZ1J1uprW90+tQjDEmIVki6UEgP4dOhdpTzV6HYowxCckSSQ9CQ4Dt8pYxxnTPEkkPAr5PhgAbY4z5LEskPRiWncHw7HQrSjTGmCuwRBKBgC/HzkiMMeYKLJFEwO+zWhJjjLkSSyQRmODL4djZFppb270OxRhjEo4lkgj4L99wtyHAxhjTlSWSCFweudVol7eMMaYrSyQRsFoSY4y5MkskEcjJTGPUkEwONVgiMcaYriyRRCjgy7FLW8YY0w1LJBEK2BBgY4zpliWSCAV8OZy60MqZ5javQzHGmIRiiSRCl2+42+UtY4z5FEskEZqQb5M3GmNMdyyRRKhoRDYpgk3eaIwxXVgiiVBmWipjhw+yMxJjjOnCEkkv+PNs5JYxxnRliaQXJjjTyauq16EYY0zCsETSCwFfDucutXPyfKvXoRhjTMJwNZGIyO0ickBEDorId7p5fryIvC4ie0TkTREpdNbfIiK7wn5aROQu57nVIrLb2We9iAx28z2EC80CbJe3jDHmE64lEhFJBR4DFgFTgOUiMqXLZj8B1qnqDcCjwA8BVHWTqk5X1enAQqAZeNXZ569U9UZnn1rgIbfeQ1cTfMGcZTfcjTHmE26ekcwFDqrqIVVtBZ4B7uyyzRTgDWd5UzfPAywFXlLVZgBVPQsgIgIMAuJ2w6JgWBbpqWJDgI0xJoybiWQscDjscZ2zLtxuYImz/BUgV0TyumyzDHg6fIWIrAGOAZOBf45VwD1JS01h3IhsOyMxxpgwXt9sfxhYICI7gQXAEaAj9KSIjAGmAa+E76SqK4ECYB/w1e4OLCLfFJHtIrK9oaEhZgHb5I3GGPNpbiaSI0BR2ONCZ91lqlqvqktUdQbwXWddU9gm9wIvqOpnZkpU1Q6Cl8vu7u7FVfVxVZ2tqrPz8/OjeydhQtPJd3baEGBjjAF3E8k2YJKIBEQkg+Alqg3hG4iIT0RCMTwClHc5xnLCLmtJ0MTQMrAY2O9S/N3y+3K41N7J0bMt8XxZY4xJWK4lElVtJzii6hWCl6CeVdUPReRREVnsbHYzcEBEPgJGAT8I7S8ifoJnNG+FHVaAtSLyPvA+MIbgaK+4udy/3S5vGWMMAGluHlxVNwIbu6z7XtjyemD9FfatpsvNeVXtBEpiHmgvhBLJoZMXKJno8zIUY4xJCF7fbE86o3KzGJSeamckxhjjsETSSykpwvi8bBu5ZYwxDkskfTAh34YAG2NMiCWSPgj4cjh8qpm2jk6vQzHGGM9ZIukDf14O7Z1K3emLXodijDGes0TSB9a/3RhjPmGJpA/8eZ8MATbGmIHOEkkfjMjJYEhWmp2RGGMMlkj6RERs8kZjjHG4WtnenwV8OWyrPu11GMaYbqgqjRda0SSZWzUtRRiek+F1GH1miaSP/L4cfr27npa2DrLSU70OxxgT5u9f3s+/vXXI6zB65Ud3T+Orc8Z5HUafWCLpo4AvB1WoPdXMtaNyvQ7HGOM429LGL7bUUDIxj9unjvE6nIg8ubWGf3vrEPfMKiIlRbwOp9cskfTR5ckbGy5YIjEmgTy77TAXWjt4ZNH1TB071OtwIjIkK42/fGYXb350goWTR3kdTq/ZzfY+8juJxG64G5M42js6WVNZzdzAiKRJIgB3TBvD6CFZrK6o8jqUPrFE0kdDstLxDc60IcDGJJDX9h7nSNNFykoCXofSK+mpKTxQPJ7Kg43sP3bW63B6zRJJFAI+mwXYmESyuqKKohGDuG1K8l0e+trccWSlp1CehGcllkiiEPDlUNVoicSYRLD7cBPba06zojhAahLesB6WncHdMwt5cVc9J89f8jqcXrFEEgW/L4eGc5c419LmdSjGDHjllVUMzkzj3tmFXofSZ2WlAVrbO3lya63XofSKJZIoTHBuuNc0NnsciTED27EzLfxuz1G+OqeI3Kx0r8Pps2vyB3PLdfn8fGsNl9o7vA4nYpZIouD32eSNxiSCdVuq6VRlRbHf61CiVlYa4OT5S/xm91GvQ4mYJZIohGYBtpFbxnjnYmsHT71byxenjKZoRLbX4UStdKKP60blsrqiCk2SOV4skUQhKz2VgqFZNnLLGA89914dTc1tlJUm15DfKxERykr97Dt6lq2HTnkdTkQskUQpYP3bjfFMZ6eyprKKaWOHMsc/3OtwYubO6WMZkZORNAWKlkii5M/L4VDD+aQ5BTWmP3nrDw183HCBVaUBRJJvyO+VZKWncv9N43h9//GkuHRuiSRKAV8OZ1vaOd1sQ4CNibfyiipG5mZyx7TkmJyxN+6fN560FOGJzdVeh9IjSyRRCvVvt8tbxsTXR8fP8fs/nOTBYj8Zaf3vo2zkkCy+fGMBz24/zJmLif1F1dXfvojcLiIHROSgiHynm+fHi8jrIrJHRN4UkUJn/S0isivsp0VE7nKee9I55gciUi4ing4aD43cskRiTHyVV1SRmZbC1+YmZw+PSJSVBGhu7eCX2xK7QNG1RCIiqcBjwCJgCrBcRKZ02ewnwDpVvQF4FPghgKpuUtXpqjodWAg0A686+zwJTAamAYOAb7j1HiJRNCKb1BRJiuuYxvQXjecv8fzOIyyZWZjUnQV7MnXsUG4KjGDt5hraOzq9DueK3DwjmQscVNVDqtoKPAPc2WWbKcAbzvKmbp4HWAq8pKrNAKq6UR3Au4Cn8yGkp6ZQNHyQnZEYE0dPvVNLa3snZSV+r0Nx3arSAEeaLvLKh8e9DuWK3EwkY4HDYY/rnHXhdgNLnOWvALkiktdlm2XA010P7lzS+jrwcncvLiLfFJHtIrK9oaGhD+FHLuCzIcDGxEtreyfrttbwhWvzmTQAmsrdev0oxudlU16ZuEOBvb5D9TCwQER2AguAI8DlCWZEZAzBS1ivdLPvvwBvq+rvuzuwqj6uqrNVdXZ+fn7sIw/j9+VQ3XjBhgAbEwe/3VNPw7lLrOonBYg9SU0RVhT72VFzml2Hm7wOp1tuJpIjQFHY40Jn3WWqWq+qS1R1BvBdZ134b+pe4AVV/dSQBRH5PpAPfNuNwHtrgi+H5tYOTpxLrqmfjUk2qsrqiiomjhzMFyb5vA4nbu6ZXURuZlrCFii6mUi2AZNEJCAiGQQvUW0I30BEfCISiuERoLzLMZbT5bKWiHwD+CNguaomxN0nf1j/dmOMe96tOsWH9WcpK+lfBYg9GZyZxlfnFLHx/aMcPXPR63A+w7VEoqrtwEMEL0vtA55V1Q9F5FERWexsdjNwQEQ+AkYBPwjtLyJ+gmc0b3U59L86225xhgZ/z633EKmAk0iqrcmVMa5aXVHF8Ox0lszseru1/3uw2I+qsnZzjdehfEaamwdX1Y3Axi7rvhe2vB5Yf4V9q/nszXlU1dWY+6Jg6CAy0lLshrsxLqppvMBr+47zZzdfQ1Z6qtfhxF3RiGxunzqap9+t5Vu3TiQ7I3E+Cr2+2d4vpKQI/rxsu7RljIue2FxNWorwwHy/16F4pqwkwJmLbTz33pGeN44jSyQxEnBGbhljYu9sSxvPbjvMl24oYNSQLK/D8cys8cO5sXAoayqq6OxMnFGilkhixO/LobaxmY4E+p9rTH/x7LbDXGjtoKxkYAz5vZJgr5IAh05e4K2P3K2P6w1LJDEywZdDa0cn9U2JN6LCmGTW0ak8sbmauf4RTCsc6nU4nrtj2hhGD8lKqKHAPSYSEfkLEek/HWNcEpq80fq3GxNbr354jLrTFykr9XsdSkJIT03hgeLxVBw8yf5jZ70OB4jsjGQUsE1EnnVm8x04g7d7IZBv/duNcUN5ZRVFIwZx25TRXoeSML42dxxZ6Smsqaj2OhQggkSiqn8LTAJWAyuAP4jI/xCRa1yOLankD84kJyPVhgAbE0N76prYVn2aFcUBUlPsO2zIsOwM7p5ZyAu7jnDyvPczakR0j8SZafeY89MODAfWi8g/uBhbUhER699uTIytrqhicGYa9872dJLvhLSyJEBreydPbvW+V0kk90j+UkR2AP8AVALTVPVPgVnA3S7Hl1T8eZZIjImVY2da+N2eo9w7u4jcLE/71yWkiSMHc/N1+fx8aw2X2jt63sFFkZyRjACWqOofqeqvQhMoOvNcfcnV6JLMBF8OdaebaW1PiCnAjElq67ZU06nKygHQc6SvVpUGOHn+Er/ZfdTTOCJJJC8Bp0IPRGSIiNwEoKr73AosGfl9OXQq1J5q9joUY5LaxdYOnnq3ltumjKJoRLbX4SSs0ok+rh01mPKKKk/bWESSSH4KnA97fN5ZZ7q4PHmjXd4yJirP76yjqbmNVaUTvA4loYkIZSUB9h49y9ZDp3rewSWRJBLRsFTnXNJKnNnCEkgokdh9EmP6rrNTKa+oYurYIczxWwlbT+6aMZYRORmeFihGkkgOici3RCTd+flL4JDbgSWjYdkZDM9Ot6JEY6Lw9h8a+LjhAqtKB1bPkb7KSk/lvpvG8fr+455dDYkkkfwHoJhgd8M64Cbgm24GlcwCvhy7tGVMFFZXVDEyN5M/nlbgdShJ4+vzxpOWIjyxudqT14+kIPGEqi5T1ZGqOkpVv6aqJ+IRXDLy+2wIsDF99dHxc/z+Dyd5YP54MtJsKsBIjRySxZdvLODZ7Yc5c7Gt5x1iLJI6kiwR+XMR+RcRKQ/9xCO4ZDTBl8Oxsy00t7Z7HYoxSWdNZRWZaSl87abxXoeSdMpKAjS3dvDstsNxf+1IUv7PgdEE+6S/BRQC59wMKpn5L4/csiHAxvTGqQutPP/eEZbMDN48Nr0zdexQbgqM4InN1bR3xLeWLZJEMlFV/ytwQVXXAn9M8D6J6Yb1bzemb556p4ZL7Z0DvudINFaVBjjSdJFX9x6P6+tGkkhCF9yaRGQqMBQY6V5IyS00nbzdJzEmcq3tnazbUsMXrs1n0qhcr8NJWrdeP4pxI7LjPhQ4kkTyuNOP5G+BDcBe4EeuRpXEcjLTGDUk0xKJMb3w2z31nDh3iTKbDiUqqSnCyhI/O2pOs+twU9xe96qJRERSgLOqelpV31bVCc7orX+LU3xJySZvNCZyqsrqiiomjhzMgmvzvQ4n6d0zu4jczDTK43hWctVE4lSx/+c4xdJvTMi3WhJjIvVu1Sk+rD9LWYkVIMbC4Mw0vjqniI3vH+Xomfi0/o7k0tb/EZGHRaRIREaEflyPLIkFfDk0XmjlTHP8x3Mbk2xWV1QxLDudr8wY63Uo/caDxX46VVm7uSYurxfJnFlfdf7752HrFLDZ1K7g8g33xgtMzx4Wl9c809zGo7/d60r9StGIbB5ZNNm+LQ5gnZ3K3/1uX8y/4arCa/uO82c3X8OgjNSYHnsgKxqRzR99bjRPv1vLt26dSHaGu9Mj9nh0Ve3zWDwRuR34/4BU4N9V9e+7PD8eKAfyCU5Vf7+q1onILcA/hm06GVimqi+KyEPAfwSuAfJV9WRf43PLhLD+7dOL4pNI1m2p5rn36pg0cjCx/LxvaevkpQ+O8YVJ+ZRO8sXuwCapvLH/BOWVVfjzsmNecT6jaBgPFvtjekwTHAq8/9g5Dp+6yHWj3R0J12MiEZEHuluvqut62C8VeAy4jeAcXdtEZIOq7g3b7CfAOlVdKyILgR8CX1fVTcB05zgjgIPAq84+lcBvgTd7it0rRSOySRHiNnnjpfYO1m2tYcG1+awtmxvzY5f8/RusrjhkiWQAW11RRcHQLP7PtxeQlmpTlySDWeOH8/q3F5ASh173kfxFzAn7+Tzw34DFEew3FzioqodUtRV4BrizyzZTgDec5U3dPA+wFHhJVZsBVHWnqlZH8PqeyUxLZezwQXG74f67PUdpOHeJstLYF3JlpqVy/7zxbDrQwMcN53vewfQ7e+vPsuVQIw8U+y2JJBERiUsSgcgmbfyLsJ8/AWYCgyM49lggfNKXOmdduN3AEmf5K0CuiOR12WYZ8HQEr/cpIvJNEdkuItsbGhp6u3vU4jUEODR0ctLIwXzBpTOG++cFJ9BbU+ldvwPjnfLKKgalp7J8zjivQzEJqi9fLy4Asfrq+zCwQER2AgsITlV/uYu9iIwBpgGv9PbAqvq4qs5W1dn5+fEfmz7BmU7e7faX74SGTrrYu8E3OJO7phfw3I4jNDW3uvIaJjGdONfChl313DO7kKHZ6V6HYxJUJLP//kZENjg/vwUOAC9EcOwjQFHY40Jn3WWqWq+qS1R1BvBdZ114Oea9wAuqmnTjaP2+HM5daufkeXc/eMsrqhgeh6GTZaUBLrZ18PS78Z9Z1Hjnya21tHZ0ssJuhpuriGRM2E/CltuBGlWti2C/bcAkEQkQTCDLgK+FbyAiPuCUU/j4CMERXOGWO+uTTvjkjfm5ma68Rk3jBV7bd5w/v3kiWenuDp2cPHoIJRPzWLu5mm98PkC6XSvv91raOvjF1hpunTySCfmRXM02A1Uknwa1wDuq+paqVgKNIuLvaSdVbQceInhZah/wrKp+KCKPikjoZv3NwAER+QgYBfwgtL/zGkUEp64nbP23RKSO4BnOHhH59wjeQ9xd7t/e4N59kic2V5OWInx9fnx6N6wqDXDsbAsb3z8al9cz3tqwq57GC62scmEQh+lfIjkj+RXBVrshHc66OT3tqKobgY1d1n0vbHk9sP4K+1bz2ZvzqOo/Af8UQdyeGjtsEOmpQpVL08mfbWnj2W2H+dINBYwakuXKa3R187UjmeDLobyiisU3FliBYj+mqpRXVjF5dC7zr+k6/sWYT4vkjCTNGb4LgLNsXWd6kJaawrgR2a6dkTy77TAXWjvi2rshxZlZdHfdGd6rPR231zXxt/njRvYfO+fqIA7Tf0SSSBrCLkUhIncCCVdNnogCLvVvb+/oZE1lNXP9I5hWODTmx7+aJTMLGZKVFvd+Bya+VldU4RucweIbC7wOxSSBSBLJfwD+i4jUikgt8DfA/+tuWP1DwJdDdeMFOjtjOwT4tb3HOdJ00ZUCxJ7kZKax/KZxvPzBMepOWzvh/uhQw3ne2H+C+24a7/ogDtM/RFKQ+LGqziNYhT5FVYtV9aD7oSU/vy+HS+2dHD3bEtPjlldWUTRiELdNGRXT40bqwfl+RIS1m6s9eX3jrjWV1WSkpnD/vPgM4jDJL5I6kv8hIsNU9byqnheR4SLyd/EILtldHgIcw8tbe+qa2FZ9mhXFAVLjNP1BVwXDBrFo6mie2XaY85diP9uw8c6Z5jbW76hj8fQC14atm/4nkktbi8KLBFX1NHCHeyH1H6FEEsvJG1dXVDE4M417ZxfG7Jh9sao0wLmWdtZvtwLF/uTpbbVcbIvvIA6T/CJJJKkicvmriYgMAuyrSgRG5WYxKD01Zmckx8608Ls9R7l3dhG5Wd5OVzFj3HBmjBvGms3VdMT4HpDxRltHJ2s3VzN/Qh5TCoZ4HY5JIpEkkieB10VklYh8A3gNWOtuWP1DSoowPi87ZiO31m2pplOVlSX+mBwvWqtKA9Q0NvPG/hNeh2Ji4OUPjnH0TIsVIJpei+Rm+4+AvwOuB64jWKlud+EiFKv+7RdbO3jq3Vq+OGU0RSOyYxBZ9G7/3GgKhmaxuuKQ16GYGFhdEWxctXDySK9DMUkm0gmTjhNsr3sPsJDglCcmAv68HGpPNdPe0RnVcZ7fWUdTc5snQ36vJC01hQeL/Ww9dIoP6894HY6Jwo6a0+w63MTKkkDceliY/uOKiURErhWR74vIfuCfCc65Jap6i6r+/3GLMMkFfDm0dyp1p/ve67qzUymvqGLa2KHM8Q+PYXTRWzZ3HNkZqZRXVHsdiolCeWUVQ7LSWDrL20EcJjld7YxkP8Gzjy+paqmq/jNhvUJMZEL926O5T/L2Hxr4uOECqxJwuoqhg9K5Z1Yhv9ldz4lzsa2XMfFxpOkiL39wjOVzx5GTGcn0e8Z82tUSyRLgKLBJRH4mIrcCifUplgT8edEPAV5dUcXI3EzumDYmVmHF1IqSAG2dnfxia63XoZg+WOcUlj5gPUdMH10xkajqi6q6DJhMsJ/6fwRGishPReSL8Qow2Y3IyWBIVlqfb7h/dPwcv//DSR4s9pORlpg9QAK+HG6dPJInt9bQ0mYnrcnkwqV2nnq3ltunjmbssEFeh2OSVCSjti6o6lOq+mWCPUB2Epxvy0RARKKavHFNZRWZaSksn5vY/bLLSgI0Xmjl17uO9LyxSRjrd9RxrqXdChBNVHr1FVdVTzu90G91K6D+qK+J5NSFVp5/7whLZhYyIiexZ+6ff00ek0fnUl5R7XqfehMbnZ3KmsoqphcNY9b4xBrEYZJLYl4r6Wf8vhzqz1zs9WWfp96p4VJ7J2UJUoB4NSLCqtIAB46fo/Jgo9fhmAi8sf8E1Y3NVoBoomaJJA4CvhxUofZU5NOut7Z3sm5LDV+4Np9Jo3JdjC52vnxjAb7BGVagmCRWV1QxZmgWt08d7XUoJslZIomDy5M39qJb4m/31HPi3KWk+raYlZ7K/fPGs+lAAx83nPc6HHMVe+vPsuVQIw8W+0lPtY8BEx37C4oDf2g6+Qj7t6sqqyuqmDhyMF+Y5HMztJi776bxZKSmsKbSOigmsvLKKgalp7J8TmIP4jDJwRJJHAzJSsc3OCPi/u3vVp3iw/qzlJUkXgFiT/JzM7lzegHP7ThCU3Or1+GYbpw418KGXfUsnVXI0GxvZ5E2/YMlkjgJ+HKoivCMpLyyiuHZ6SyZOdblqNxRVhrgYlsHT79rvUoS0ZNba2nt6EyYWaRN8rNEEieRDgGubWzm1b3H+dpN45K2X/b1Y4ZQfE0eazdX0xblZJUmtlraOvjF1hoWTh7JhPzBXodj+glLJHHi9+XQcO4S51rarrrdms1VpIrwwHx/fAJzyarSAMfOtvDSB8e8DsWE2bC7nsYLrUk1iMMkPkskcTLBueFe03jlIcDnWtr41fY6vnTDGEYNyYpXaK645bqRBHw5rK6osgLFBKEanEV68uhciq/J8zoc04+4mkhE5HYROSAiB0XkO908P15EXheRPSLypogUOutvEZFdYT8tInKX81xARN5xjvlLEUnskm+HP4L+7b/cdpjzl9pZVTohXmG5JiVFWFniZ/fhJt6rPe11OAbY/HEj+4+dS8pBHCaxuZZIRCQVeAxYBEwBlovIlC6b/QRYp6o3AI8CPwRQ1U2qOl1VpxOcyr4ZeNXZ50fAP6rqROA0sMqt9xBLoVmArzR5Y0en8sTmaub6RzCtcGg8Q3PN3TMLGZKVZr1KEkR5RRV5ORksnl7gdSimn3HzjGQucFBVD6lqK/AMcGeXbaYAbzjLm7p5HmAp8JKqNkvwa9RCYL3z3FrgrphH7oKs9FQKhmZd8Yb7a3uPUXf6ImWl/vgG5qKczDSWzx3HSx8cpe505FX9JvYONZzn9f0nuG/e+KQdxGESl5uJZCwQPv6zzlkXbjfBvicAXwFyRaTrxdtlwNPOch7QpKrtVzlmwgrkX3nk1uqKKopGDOK2Kf1ruooHi/2ICGudnhfGG2sqq8lITeHr88Z7HYrph7y+2f4wsEBEdgILgCOEdWEUkTHANOCV3h5YRL4pIttFZHtDQ0Os4o2KP6/7RLKnrolt1adZURwgtZ/1yy4YNohFU0fzjHP/x8TfmeY21u+oY/H0AvJzM70Ox/RDbiaSI0BR2ONCZ91lqlqvqktUdQbwXWddU9gm9wIvqGpozGwjMExEQv1AP3PMsGM/rqqzVXV2fn5+9O8mBgK+HM5cbOP0hU9XfJdXVDE4M417Z/fPftllpQHOtbSzfrsVKHrh6W21XGzrsJ4jxjVuJpJtwCRnlFUGwUtUG8I3EBGfiIRieAQo73It3xWCAAAUiUlEQVSM5XxyWQsNjiPdRPC+CcCDwK9diN0VgW5Gbh0/28Jv9xzl3tlF5Gb1z+kqZo4bzoxxw1izuZrOThsKHE9tHZ2s3VzN/Al5TCkY4nU4pp9yLZE49zEeInhZah/wrKp+KCKPishiZ7ObgQMi8hEwCvhBaH8R8RM8o3mry6H/Bvi2iBwkeM9ktVvvIdZCiSR85Na6LdV0qLKin/fLLisJUNPYzOv7T3gdyoDy8gfHOHqmhTIrQDQuSut5k75T1Y3Axi7rvhe2vJ5PRmB13beabm6kq+ohgiPCkk7RiGxSU+TyfZKLrR08+U4tX5wyinF52R5H565FU0dTMDSL1RWHuG3KKK/DGTBWV1Thz8vm1skjvQ7F9GNe32wfUNJTUygaPuhyInlh5xGamtv6RQFiT9JSU3iw2M/WQ6f4sP6M1+EMCO/VnmbX4SZWlgRI6WeDOExisUQSZ6HJG1WV8soqpo4dwhz/wOiXvWzOOAalp1qBYpysrqgiNyuNpbP65yAOkzgskcSZ35dDdeMF3vqogYMnzrOqdOBMVzE0O517Zhfym931nDjX4nU4/dqRpou8/MExls8dR06mq1ewjbFEEm8TfDk0t3bwDy8fYGRuJn88bWBNV7Gi2E9rRye/2FrrdSj92rrN1agqD8y3AkTjPkskcRaavHHv0bM8MH88GWkD63/BhPzB3Dp5JE9uraGlraPnHUyvXbjUzlPv1rJo6hgKh/fvQRwmMQysT7EEEBoCnJmWwtduGpjfFleVBmi80Mqvd3VbS5qQPm4473UIEVu/o45zLe025NfEjSWSOCsYOoihg9JZOquQETlJMQN+zM2/Jo/Jo3Mpr6hOil4lG98/yq3/8y1e33fc61B61OnMIn1j0TBmjhvmdThmgLBEEmcpKcJLf/l5vvflrjPqDxwiQllpgAPHz1F5sNHrcHr0+NuHAPjZ7w95HEnP3th/gqqTFwbUIA7jPUskHigYNojMtIE9lffiGwvwDc6gvLLK61CuKlSLMXl0blLUwJRXVjFmaBaLpvavWaRNYrNEYjyRlZ7KfTeN5439JxL6/kOoFmPNyjlkZyR2Dcze+rNs/riRB+b7SU+1f9omfuyvzXjm/nnjyUhNYU2CnpWE12KMGTqIpbMSuwamvLKKQempLJ9b1PPGxsSQJRLjmfzcTBZPL+C5HUdoam7teYc4W+c043rQmVBzZUmAts7ErIFpOHeJDbvquXvWWIZlD8xBHMY7lkiMp8pKAlxs6+DpdxOrV0moFuP2qaMZO2wQEBy6nag1ML/YWkNrRycrreeI8YAlEuOpKQVDKL4mj3Vbqmnr6PQ6nMuee8+pxejywVxWEqyB2bCr3qPIPqulrYMn36lh4eSRXJM/2OtwzABkicR4rqwkwNEzLbz0wTGvQwGCtRhrKquZXjSMWeM/PaHm5RqYyqqEqYHZsLuek+dbrQOi8YwlEuO5hZNHEvDlsLoiMT6cQ7UY3VWGh2pg9h9LjBoYVaW8oorJo3MpmZjndThmgLJEYjyXkiKsLPGz+3AT79U2eR1Oj7UYiVQDs+XjRvYfO0dZiRUgGu9YIjEJ4e6ZhQzJSqO8wtsP51AtxoPFV67FyEpP5f55iVEDs7qiirycDBZPH1izSJvEYonEJISczDSWzx3HSx8cpe50s2dxrAnVYswZd9Xt7rvJ+xqYqpMXeH3/Ce6bN56s9IE9U4LxliUSkzAeKPYjIqzbUuPJ6zecu8Svd9WzdFYhQ7PTr7ptfm4md3pcA7OmsoqM1BTun3f1pGeM2yyRmIQxdtggbp86mqffreXCpfa4v36oFmNFiT+i7Vd6WANzprmNX22v48s3FjAyNyvur29MOEskJqGsKg1wrqWd9Tvq4vq6fanF8LIG5plttVxs66Cs1B/X1zWmO5ZITEKZOW4404uGsaayis7O+A0FDtVirOplM6hVpfGvgWnv6GTt5mrmTRjB5wqGxu11jbkSSyQm4awqDVDd2Mzr+0/E5fXCazGKr+ldLcYt18W/BublD49Rf6aFVaUT4vJ6xvTEEolJOIumjqZgaFbchgJHU4vhRQ3M6ooqxudlc+vkkXF5PWN64moiEZHbReSAiBwUke908/x4EXldRPaIyJsiUhj23DgReVVE9onIXhHxO+sXish7IvKBiKwVkTQ334OJv7TUFB4o9rPlUGNcGklFW4sRzxqY92pPs7O2iZXFflJSrADRJAbXEomIpAKPAYuAKcByEenaX/YnwDpVvQF4FPhh2HPrgB+r6vXAXOCEiKQAa4FlqjoVqAEedOs9GO8snzOOQenuN5KKRS1GPGtgyp1GW/fMtp4jJnG4eUYyFzioqodUtRV4BrizyzZTgDec5U2h552Ek6aqrwGo6nlVbQbygFZV/cjZ5zXgbhffg/HI0Ox07pntfiOpWNVixKMG5kjTRV5yGm3lZNqJuEkcbiaSsUD4APs6Z1243cASZ/krQK6I5AHXAk0i8ryI7BSRHztnOCeBNBGZ7eyzFLCvZv3UimI/rR3uNZKKZS1GPGpg1m2pRlV5YP54V45vTF95fbP9YWCBiOwEFgBHgA4gDfi88/wcYAKwQoPDYpYB/ygi7wLnnO0/Q0S+KSLbRWR7Q0OD++/ExNyE/MGuNpIK1WL0dsjvlbhZA3PhUjtPv1PLoqljKByeHfPjGxMNNxPJET59tlDorLtMVetVdYmqzgC+66xrInj2ssu5LNYOvAjMdJ7foqqfV9W5wNvAR3RDVR9X1dmqOjs/Pz/W783ESVmpO42kQrUY8yfkMaVgSEyOOXPccGaMc6cG5rn36jjb0m4FiCYhuZlItgGTRCQgIhkEzyQ2hG8gIj7nBjrAI0B52L7DRCSUARYCe519Rjr/zQT+BvhXF9+D8VixS42kQrUY3fUciUZZSexrYEKNtm4sGsbMccN73sGYOHMtkThnEg8BrwD7gGdV9UMReVREFjub3QwcEJGPgFHAD5x9Owhe1npdRN4HBPiZs89fi8g+YA/wG1UN3aw3/VB4I6nNH8eukdTqiir8LtRiuFEDs+lAsNHWqlLrOWISk6v3SFR1o6peq6rXqGooSXxPVTc4y+tVdZKzzTdU9VLYvq+p6g2qOk1VVzgjv1DVv1bV61X1OlX9327GbxJDqJHU6hh9OF+uxSgJxLwWIy01hQdjXAPTU6MtY7zm9c12Y3qUlZ7KfTfFrpFUqBZj6azCnjfug2UxrIHZd/QslQcbeWD+lRttGeM1+8s0SeH+ecFGUk9UVkd1nHjUYsSyBqa8wmm0NddGuZvEZYnEJIX83EwWTy9g/Y66qBpJxasWIxY1MCfPBxtt3T1rLMOyM2IYnTGxZYnEJI0yp5HUM9v61kgqnrUYsaiBCTXaWlkS25FlxsSaJRKTNEKNpNZu7lsjqecv12LE54N5VRQ1MJfaO/jF1t412jLGK5ZITFIpK+lbI6nOTqX8ci3GMJei+7T5UdTAbNgVbLRVZmcjJglYIjFJZeHkkfjzsntdp+FFLUZfa2BUldUVVVw3KpeSib1rtGWMFyyRmKQSbCQVYNfhJnbUnI54P69qMfpSA7PlkNNoq9RvBYgmKVgiMUln6azeNZLyshajLzUw5U6jrTund50s25jEZInEJJ3eNpJaU+ltLUZvamBi0WjLmHizRGKSUqSNpE6ev8SLHtdi9KYG5onKKtJTom+0ZUw8WSIxSSnSRlK/2FpDa7v3tRiR1MCcudjGr3bEptGWMfFkicQkrZ4aSSVSLUYkNTC/3FZLc2uH9RwxSccSiUlaM8cNZ3rRlRtJJVotxtVqYIKNtmqYN2EEnysY6kF0xvSdJRKT1FaVBhtJvdGlkZRqsABx8ujEqcVYOHkkAV9Ot6PNXvnwOEeaLrKqdIIHkRkTHUskJqndPnU0Y4ZmfaZOY8uhRvYdPUtZSeI0gwrWwPi7rYFZXXGI8XnZLIxxoy1j4sESiUlq6WGNpPbWn728PlSLsXh6gYfRfdbdM50amMpPEt/O2tO8V9vEymI/qTFutGVMPFgiMUlveaiRlPPhnMi1GKEamJc/OMaRposAlFdWk5uVxj2zreeISU6WSEzSG5qdztJZhWzYFWwklei1GA8U+wFYu7ma+qaLbHz/KMvmFLnWaMsYt9lfrukXVpb4+fnWGn765scJX4sRXgPT0taBqvKgk1yMSUZ2RmL6hVAjqTWV1UlRixGqgVm3pSYujbaMcZMlEtNvhBpWJUMtRqgGBkj4pGdMT+zSluk3iq/J41sLJ/LFz8V3qvi++v6Xp/D2RyeZOW6416EYExVLJKbfEBG+/cXrvA4jYjPGDWeGJRHTD9ilLWOMMVFxNZGIyO0ickBEDorId7p5fryIvC4ie0TkTREpDHtunIi8KiL7RGSviPid9beKyHsisktEKkRkopvvwRhjzNW5lkhEJBV4DFgETAGWi8iULpv9BFinqjcAjwI/DHtuHfBjVb0emAuEJlP6KXCfqk4HngL+1q33YIwxpmdunpHMBQ6q6iFVbQWeAe7sss0U4A1neVPoeSfhpKnqawCqel5VQ63wFBjiLA8F6t17C8YYY3riZiIZC4R38alz1oXbDSxxlr8C5IpIHnAt0CQiz4vIThH5sXOGA/ANYKOI1AFfB/7etXdgjDGmR17fbH8YWCAiO4EFwBGgg+Boss87z88BJgArnH3+CrhDVQuBNcD/6u7AIvJNEdkuItsbGhpcfRPGGDOQuZlIjgDhs9AVOusuU9V6VV2iqjOA7zrrmgievexyLou1Ay8CM0UkH7hRVd9xDvFLoLi7F1fVx1V1tqrOzs/Pj+kbM8YY8wk3E8k2YJKIBEQkA1gGbAjfQER8IhKK4RGgPGzfYU7iAFgI7AVOA0NF5Fpn/W3APhffgzHGmB64VpCoqu0i8hDwCpAKlKvqhyLyKLBdVTcANwM/FBEF3gb+3Nm3Q0QeBl6XYFeiHcDPnGP+CfCciHQSTCxlPcWyY8eOkyJS08u34ANO9nIfryRTrJBc8SZTrJBc8SZTrJBc8cYq1vGRbCSqn+11bUBEtqvqbK/jiEQyxQrJFW8yxQrJFW8yxQrJFW+8Y/X6ZrsxxpgkZ4nEGGNMVCyRXNnjXgfQC8kUKyRXvMkUKyRXvMkUKyRXvHGN1e6RGGOMiYqdkRhjjInKgE8kEcxQnCkiv3Sefyc0C7EXIoj1285MyXucWZUjGrrnlp7iDdvubhFREfFsREwksYrIvc7v90MReSreMXaJpae/hXEissmZYmiPiNzhUZzlInJCRD64wvMiIv/kvI89IjIz3jF2iaeneO9z4nxfRDaLyI3xjjEslqvGGrbdHBFpF5GlrgWjqgP2h2B9y8cEp2DJIDj315Qu2/wZ8K/O8jLglwkc6y1AtrP8p17FGmm8zna5BGuItgKzEzVWYBKwExjuPB6ZyL9bgtfI/9RZngJUexTrF4CZwAdXeP4O4CVAgHnAO179XiOMtzjsb2CRl/H2FGvY38obwEZgqVuxDPQzkkhmKL4TWOssrwdudYok463HWFV1k34yS/JWgtPSeCWS3y3Afwd+BLTEM7guIon1T4DHVPU0gKqewDuRxJsQs2Sr6tvAqatscifBVhKqqlsJzmgxJj7RfVZP8arq5tDfAB7/G4vgdwvwF8BzfNKGwxUDPZFEMkPx5W00OO/XGSAvLtFdIQ5Hd7GGW0Xwm55XeozXuYxRpKq/i2dg3Yjkd3stcK2IVIrIVhG5PW7RfVYk8f434H5nluyNBD9QElFv/64Tidf/xq5KRMYSnFX9p26/lvVs74dE5H5gNsEZlROSM8fa/+KTWZ0TXRrBy1s3E/wW+raITNPgJKOJaDnwhKr+TxGZD/xcRKaqaqfXgfUHInILwURS6nUsV/G/gb9R1U63L6IM9ETS4wzFYdvUiUgawcsEjfEJr9s4QrqLFRH5fwjOpLxAVS/FKbbu9BRvLjAVeNP5Ix8NbBCRxaq6PW5RBkXyu60jeD28DagSkY8IJpZt8QnxUyKJdxVwO4CqbhGRLILzL3l5Sa47Ef1dJxIRuQH4d2CRqnrxWRCp2cAzzr8vH3CHiLSr6ouxfqGBfmmrxxmKnccPOstLgTfUuYsVZ5HMpjwD+DdgscfX8KGHeFX1jKr6VNWvqn6C15u9SCI9xup4keDZCCLiI3ip61A8gwwTSby1wK0AInI9kAUkYmOeDcADzuitecAZVT3qdVBXIiLjgOeBr6vqR17HczWqGgj797Ue+DM3kggM8DMSjWyG4tUELwscJHhja1kCx/pjYDDwK+dbSK2qLk7geBNChLG+AnxRRPYSbL721159G40w3v8E/ExE/orgjfcVXnwBEpGnCSZgn3O/5vtAuvM+/pXg/Zs7gINAM7Ay3jGGiyDe7xG8R/ovzr+xdvVoIscIYo1fLN58uTbGGNNfDPRLW8YYY6JkicQYY0xULJEYY4yJiiUSY4wxUbFEYowxJiqWSIyJIRG5y5nJeLLz2B/B7Kw9bmNMIrNEYkxsLQcqnP8aMyBYIjEmRkRkMMG5l1bRTeGqiKwQkV+LyJsi8gcR+X7Y06ki8jOn18mrIjLI2edPRGSbiOwWkedEJDs+78aYyFkiMSZ27gRedqbOaBSRWd1sMxe4G7gBuEc+aeY1ieA09Z8DmpxtAJ5X1TmqeiOwj2CSMiahWCIxJnaWE+wNgvPf7i5vvaaqjap6keCcTaHZY6tUdZezvAPwO8tTReT3IvI+cB/wOVciNyYKA3quLWNiRURGAAuBaSKiBOfAUuCxLpt2nZMo9Dh8puYOYJCz/ARwl6ruFpEVOBNHGpNI7IzEmNhYCvxcVcc7M64WAVV8eop0gNtEZIRzD+QuoLKH4+YCR0UkneAZiTEJxxKJMbGxHHihy7rngEe6rHvXWb8HeC6CafP/K/AOwYSzPwZxGhNzNvuvMXHiXJqaraoPeR2LMbFkZyTGGGOiYmckxhhjomJnJMYYY6JiicQYY0xULJEYY4yJiiUSY4wxUbFEYowxJiqWSIwxxkTl/wLcM25hWagy0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i['alpha'] for i in clf.cv_results_['params']], clf.cv_results_['mean_test_score'])\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9619883040935673\n",
      "\n",
      "Confusion Matrix:\n",
      " [[218   5]\n",
      " [  8 111]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.98      0.97       223\n",
      "        True       0.96      0.93      0.94       119\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       342\n",
      "   macro avg       0.96      0.96      0.96       342\n",
      "weighted avg       0.96      0.96      0.96       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "y_pred = clf.predict(count_test)\n",
    "evaluate_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model - hyperparamter tuning using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('counts', CountVectorizer()),('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "                    'counts__min_df': [0, 0.02, 0.05],\n",
    "                    'classifier__alpha': np.arange(0.01,1.5,.1)\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_grid = GridSearchCV(pipeline, parameter_space, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocess...one, vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'counts__min_df': [0, 0.02, 0.05], 'classifier__alpha': array([0.01, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91, 1.01,\n",
       "       1.11, 1.21, 1.31, 1.41])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier via GridSearcCV\n",
    "pipe_grid.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score  0.9682080924855492 , Best params: {'classifier__alpha': 0.11, 'counts__min_df': 0}\n"
     ]
    }
   ],
   "source": [
    "# View the accuracy score\n",
    "print('Best score ', pipe_grid.best_score_, ', Best params:',  pipe_grid.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9649122807017544\n",
      "\n",
      "Confusion Matrix:\n",
      " [[218   5]\n",
      " [  7 112]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.98      0.97       223\n",
      "        True       0.96      0.94      0.95       119\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       342\n",
      "   macro avg       0.96      0.96      0.96       342\n",
      "weighted avg       0.96      0.96      0.96       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "y_pred = pipe_grid.predict(X_test)\n",
    "evaluate_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing your model with GridSearchCV and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('counts', CountVectorizer()), \n",
    "                 ('preprocessing', StandardScaler()), \n",
    "                 ('classifier', SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'counts':[TfidfVectorizer(), CountVectorizer()], \n",
    "     'classifier': [MultinomialNB()], \n",
    "     'preprocessing': [None],\n",
    "     'classifier__alpha': np.arange(0.01,1.5,.1)},\n",
    "    {'counts':[TfidfVectorizer(), CountVectorizer()], \n",
    "     'classifier': [SVC()], \n",
    "     'preprocessing': [StandardScaler(with_mean=False), None],\n",
    "     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    {'counts':[TfidfVectorizer(), CountVectorizer()], \n",
    "     'classifier': [RandomForestClassifier(n_estimators=100)],\n",
    "     'preprocessing': [None], \n",
    "     'classifier__max_features': [1, 2], \n",
    "     'classifier__max_depth': [1,2]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, param_grid, cv=3,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        str...f', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'counts': [TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_id...se)], 'preprocessing': [None], 'classifier__max_features': [1, 2], 'classifier__max_depth': [1, 2]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.11, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.11, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.21000000000000002, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.21000000000000002, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.31000000000000005, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.31000000000000005, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.41000000000000003, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.41000000000000003, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.51, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.51, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.6100000000000001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.6100000000000001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.7100000000000001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.7100000000000001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.81, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.81, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.91, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 0.91, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.11, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.11, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.2100000000000002, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.2100000000000002, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.31, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.31, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.4100000000000001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'classifier__alpha': 1.4100000000000001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.001, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.01, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 0.1, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 1, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.001, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.001, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.01, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.01, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 0.1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 10, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 10, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 100, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': StandardScaler(copy=True, with_mean=False, with_std=True)}\n",
      "***\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 100, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 1, 'classifier__max_features': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 1, 'classifier__max_features': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 1, 'classifier__max_features': 2, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 1, 'classifier__max_features': 2, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 2, 'classifier__max_features': 1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 2, 'classifier__max_features': 1, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 2, 'classifier__max_features': 2, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "***\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'classifier__max_depth': 2, 'classifier__max_features': 2, 'counts': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'preprocessing': None}\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "for param_combo in clf.cv_results_['params']:\n",
    "    print(param_combo)\n",
    "    print('***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 0.1, 'counts': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'preprocessing': None}\n",
      "\n",
      "Best cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\\n{}\\n\".format(clf.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9736842105263158\n",
      "\n",
      "Confusion Matrix:\n",
      " [[218   5]\n",
      " [  4 115]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98       223\n",
      "        True       0.96      0.97      0.96       119\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       342\n",
      "   macro avg       0.97      0.97      0.97       342\n",
      "weighted avg       0.97      0.97      0.97       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "evaluate_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
